# -*- coding: utf-8 -*-

import os
import numpy as np
import networkx as nx
from tqdm import tqdm
import pickle
from collections import Counter

def merge_similar_large_clusters(clusters, facial_encodings, min_cluster_size=10, 
                                similarity_threshold=0.55, max_cross_sample_check=20):
    """
    Phase 1: æª¢æŸ¥ä¸¦åˆä½µç›¸ä¼¼çš„å¤§clusters
    
    Args:
        clusters: åŸå§‹clusters
        facial_encodings: äººè‡‰ç·¨ç¢¼å­—å…¸
        min_cluster_size: è¢«è¦–ç‚º"å¤§cluster"çš„æœ€å°size
        similarity_threshold: åˆä½µé–¾å€¼ï¼ˆå¤§clusteréœ€è¦æ›´é«˜ç›¸ä¼¼åº¦ï¼‰
        max_cross_sample_check: æ¯å€‹clusteræœ€å¤šå–æ¨£æœ¬æ•¸é€²è¡Œæ¯”å°
        
    Returns:
        åˆä½µå¾Œçš„clusterså’Œmerge actions
    """
    print(f"ğŸ” Phase 1: åˆ†æå¤§clustersï¼ˆsize >= {min_cluster_size}ï¼‰...")
    
    #é€²åº¦é¡¯ç¤ºæ”¹é€²
    from tqdm import tqdm
    
    large_clusters = [(i, cluster) for i, cluster in enumerate(clusters) if len(cluster) >= min_cluster_size]
    print(f"   æ‰¾åˆ° {len(large_clusters)} å€‹å¤§clusters")
    
    if len(large_clusters) < 2:
        print("   å¤§clustersæ•¸é‡ä¸è¶³ï¼Œè·³éPhase 1")
        return clusters, []
    
    # ä½¿ç”¨tqdmé¡¯ç¤ºæ¯”è¼ƒé€²åº¦
    total_comparisons = len(large_clusters) * (len(large_clusters) - 1) // 2
    merge_candidates = []
    
    pbar = tqdm(total=total_comparisons, desc="æ¯”è¼ƒå¤§clusters")
    for i in range(len(large_clusters)):
        for j in range(i+1, len(large_clusters)):
            # æ¯”è¼ƒé‚è¼¯...
            pbar.update(1)
    pbar.close()

    # è­˜åˆ¥å¤§clusters
    large_clusters = []
    small_clusters = []
    
    for i, cluster in enumerate(clusters):
        if len(cluster) >= min_cluster_size:
            large_clusters.append((i, cluster))
        else:
            small_clusters.append((i, cluster))
    
    print(f"   æ‰¾åˆ° {len(large_clusters)} å€‹å¤§clustersï¼Œ{len(small_clusters)} å€‹å°clusters")
    
    if len(large_clusters) < 2:
        print("   å¤§clustersæ•¸é‡ä¸è¶³ï¼Œè·³éPhase 1")
        return clusters, []
    
    # è¨ˆç®—å¤§clustersä¹‹é–“çš„ç›¸ä¼¼åº¦
    merge_candidates = []
    
    for i in range(len(large_clusters)):
        for j in range(i+1, len(large_clusters)):
            idx_i, cluster_i = large_clusters[i]
            idx_j, cluster_j = large_clusters[j]
            
            print(f"   æ¯”è¼ƒ Cluster {idx_i} ({len(cluster_i)} faces) vs Cluster {idx_j} ({len(cluster_j)} faces)")
            
            # è¨ˆç®—cross-clusterç›¸ä¼¼åº¦
            similarities = calculate_cross_cluster_similarity(
                cluster_i, cluster_j, facial_encodings, max_cross_sample_check
            )
            
            # å¤šé‡åˆ¤æ–·æ¢ä»¶
            centroid_sim = similarities['centroid']
            max_pairwise = similarities['max_pairwise']
            avg_pairwise = similarities['avg_pairwise']
            high_sim_ratio = similarities['high_similarity_ratio']  # é«˜ç›¸ä¼¼åº¦pairsçš„æ¯”ä¾‹
            
            print(f"      è³ªå¿ƒç›¸ä¼¼åº¦: {centroid_sim:.3f}")
            print(f"      æœ€é«˜é…å°ç›¸ä¼¼åº¦: {max_pairwise:.3f}")
            print(f"      å¹³å‡é…å°ç›¸ä¼¼åº¦: {avg_pairwise:.3f}")
            print(f"      é«˜ç›¸ä¼¼åº¦æ¯”ä¾‹: {high_sim_ratio:.3f}")
            
            # æ±ºå®šæ˜¯å¦åˆä½µï¼ˆä½¿ç”¨å¤šé‡æ¢ä»¶ï¼‰
            should_merge = (
                centroid_sim > similarity_threshold and
                (max_pairwise > similarity_threshold + 0.1 or  # æœ‰éå¸¸ç›¸ä¼¼çš„faces
                 (avg_pairwise > similarity_threshold - 0.05 and high_sim_ratio > 0.3))  # æ•´é«”ç›¸ä¼¼ä¸”æœ‰è¶³å¤ é«˜ç›¸ä¼¼åº¦pairs
            )
            
            if should_merge:
                merge_candidates.append({
                    'cluster_i': idx_i,
                    'cluster_j': idx_j,
                    'centroid_sim': centroid_sim,
                    'avg_pairwise': avg_pairwise,
                    'confidence': (centroid_sim + avg_pairwise + high_sim_ratio) / 3
                })
                print(f"      âœ… æ¨™è¨˜ç‚ºåˆä½µå€™é¸")
            else:
                print(f"      âŒ ä¸ç¬¦åˆåˆä½µæ¢ä»¶")
    
    # åŸ·è¡Œåˆä½µï¼ˆæŒ‰confidenceæ’åºï¼‰
    merge_candidates.sort(key=lambda x: x['confidence'], reverse=True)
    
    # ä½¿ç”¨Union-Findä¾†è™•ç†å¤šè·¯åˆä½µ
    cluster_groups = {}
    for i, cluster in enumerate(clusters):
        cluster_groups[i] = i
    
    def find_root(x):
        if cluster_groups[x] != x:
            cluster_groups[x] = find_root(cluster_groups[x])
        return cluster_groups[x]
    
    def union(x, y):
        root_x, root_y = find_root(x), find_root(y)
        if root_x != root_y:
            cluster_groups[root_y] = root_x
    
    merge_actions = []
    
    for candidate in merge_candidates:
        i, j = candidate['cluster_i'], candidate['cluster_j']
        
        # æª¢æŸ¥æ˜¯å¦å·²ç¶“åœ¨åŒä¸€çµ„
        if find_root(i) != find_root(j):
            union(i, j)
            merge_actions.append({
                'type': 'large_cluster_merge',
                'cluster_i': i,
                'cluster_j': j,
                'confidence': candidate['confidence'],
                'centroid_sim': candidate['centroid_sim']
            })
            print(f"   âœ… åˆä½µ Cluster {i} å’Œ Cluster {j} (confidence: {candidate['confidence']:.3f})")
    
    # é‡å»ºclusters
    group_to_cluster = {}
    for i, cluster in enumerate(clusters):
        root = find_root(i)
        if root not in group_to_cluster:
            group_to_cluster[root] = []
        group_to_cluster[root].extend(cluster)
    
    new_clusters = list(group_to_cluster.values())
    
    print(f"âœ… Phase 1 å®Œæˆ: {len(clusters)} â†’ {len(new_clusters)} clusters")
    return new_clusters, merge_actions


def calculate_cross_cluster_similarity(cluster1, cluster2, facial_encodings, max_samples=20):
    """
    è¨ˆç®—å…©å€‹clustersä¹‹é–“çš„è©³ç´°ç›¸ä¼¼åº¦æŒ‡æ¨™
    """
    # æ¡æ¨£ä»¥é¿å…è¨ˆç®—é‡éå¤§
    sample1 = cluster1[:max_samples] if len(cluster1) <= max_samples else \
              np.random.choice(cluster1, max_samples, replace=False).tolist()
    sample2 = cluster2[:max_samples] if len(cluster2) <= max_samples else \
              np.random.choice(cluster2, max_samples, replace=False).tolist()
    
    # è¨ˆç®—è³ªå¿ƒç›¸ä¼¼åº¦
    center1 = calculate_cluster_center(sample1, facial_encodings)
    center2 = calculate_cluster_center(sample2, facial_encodings)
    centroid_sim = np.dot(center1, center2) if center1 is not None and center2 is not None else 0
    
    # è¨ˆç®—pairwiseç›¸ä¼¼åº¦
    similarities = []
    for path1 in sample1:
        for path2 in sample2:
            sim = np.dot(facial_encodings[path1], facial_encodings[path2])
            similarities.append(sim)
    
    if not similarities:
        return {'centroid': 0, 'max_pairwise': 0, 'avg_pairwise': 0, 'high_similarity_ratio': 0}
    
    similarities = np.array(similarities)
    
    return {
        'centroid': centroid_sim,
        'max_pairwise': np.max(similarities),
        'avg_pairwise': np.mean(similarities),
        'high_similarity_ratio': np.sum(similarities > 0.7) / len(similarities)  # é«˜ç›¸ä¼¼åº¦æ¯”ä¾‹
    }


def merge_small_clusters_intelligently(clusters, facial_encodings, small_cluster_threshold=50,
                                     merge_threshold=0.45, safety_checks=True):
    """
    Phase 2: æ™ºèƒ½åˆä½µå°clustersï¼Œå¸¶æœ‰å®‰å…¨æª¢æŸ¥
    
    Args:
        clusters: Phase 1è™•ç†å¾Œçš„clusters
        facial_encodings: äººè‡‰ç·¨ç¢¼å­—å…¸
        small_cluster_threshold: å°clusterçš„é–¾å€¼
        merge_threshold: åˆä½µé–¾å€¼
        safety_checks: æ˜¯å¦å•Ÿç”¨å®‰å…¨æª¢æŸ¥
        
    Returns:
        æœ€çµ‚clusterså’Œmerge actions
    """
    print(f"ğŸ” Phase 2: è™•ç†å°clustersï¼ˆsize < {small_cluster_threshold}ï¼‰...")
    
    # è­˜åˆ¥å¤§å°clusters
    large_clusters = []
    small_clusters = []
    
    for i, cluster in enumerate(clusters):
        if len(cluster) >= small_cluster_threshold:
            large_clusters.append((i, cluster))
        else:
            small_clusters.append((i, cluster))
    
    print(f"   å¤§clusters: {len(large_clusters)}ï¼Œå°clusters: {len(small_clusters)}")
    
    if not small_clusters:
        print("   æ²’æœ‰å°clusterséœ€è¦è™•ç†")
        return clusters, []
    
    # ç‚ºæ¯å€‹å°clusterå°‹æ‰¾æœ€ä½³åˆä½µç›®æ¨™
    merge_proposals = []
    
    for small_idx, small_cluster in small_clusters:
        best_match = None
        best_similarity = 0
        
        print(f"   åˆ†æå°cluster {small_idx} ({len(small_cluster)} faces)...")
        
        # èˆ‡æ‰€æœ‰å¤§clustersæ¯”è¼ƒ
        for large_idx, large_cluster in large_clusters:
            similarities = calculate_inter_cluster_similarity(
                small_cluster, large_cluster, facial_encodings
            )
            
            combined_score = (
                similarities['centroid'] * 0.4 +
                similarities['max_pairwise'] * 0.3 +
                similarities['avg_pairwise'] * 0.3
            )
            
            if combined_score > best_similarity:
                best_similarity = combined_score
                best_match = large_idx
                
        if best_match is not None and best_similarity > merge_threshold:
            # å®‰å…¨æª¢æŸ¥
            if safety_checks:
                is_safe = perform_safety_checks(
                    small_cluster, clusters[best_match], facial_encodings, merge_threshold
                )
                if not is_safe:
                    print(f"      âŒ å®‰å…¨æª¢æŸ¥å¤±æ•—ï¼Œè·³éåˆä½µ")
                    continue
            
            merge_proposals.append({
                'small_cluster': small_idx,
                'target_cluster': best_match,
                'similarity': best_similarity,
                'small_size': len(small_cluster)
            })
            print(f"      âœ… å»ºè­°åˆä½µåˆ° Cluster {best_match} (ç›¸ä¼¼åº¦: {best_similarity:.3f})")
        else:
            print(f"      âŒ æ²’æ‰¾åˆ°åˆé©çš„åˆä½µç›®æ¨™ (æœ€é«˜ç›¸ä¼¼åº¦: {best_similarity:.3f})")
    
    # åŸ·è¡Œåˆä½µ
    new_clusters = [cluster.copy() for cluster in clusters]
    merge_actions = []
    
    # æŒ‰similarityæ’åºï¼Œå„ªå…ˆè™•ç†é«˜ç›¸ä¼¼åº¦çš„åˆä½µ
    merge_proposals.sort(key=lambda x: x['similarity'], reverse=True)
    
    merged_indices = set()
    
    for proposal in merge_proposals:
        small_idx = proposal['small_cluster']
        target_idx = proposal['target_cluster']
        
        if small_idx not in merged_indices:
            # åŸ·è¡Œåˆä½µ
            new_clusters[target_idx].extend(new_clusters[small_idx])
            new_clusters[small_idx] = []  # æ¸…ç©ºè¢«åˆä½µçš„cluster
            merged_indices.add(small_idx)
            
            merge_actions.append({
                'type': 'small_cluster_merge',
                'source': small_idx,
                'target': target_idx,
                'faces_added': proposal['small_size'],
                'similarity': proposal['similarity']
            })
            print(f"   âœ… å·²åˆä½µ Cluster {small_idx} â†’ Cluster {target_idx}")
    
    # ç§»é™¤ç©ºclusters
    final_clusters = [cluster for cluster in new_clusters if len(cluster) > 0]
    
    print(f"âœ… Phase 2 å®Œæˆ: åˆä½µäº† {len(merge_actions)} å€‹å°clusters")
    print(f"   æœ€çµ‚clustersæ•¸é‡: {len(final_clusters)}")
    
    return final_clusters, merge_actions


def perform_safety_checks(small_cluster, target_cluster, facial_encodings, base_threshold):
    """
    åŸ·è¡Œå®‰å…¨æª¢æŸ¥ä»¥é˜²æ­¢éŒ¯èª¤åˆä½µ
    
    Args:
        small_cluster: è¦åˆä½µçš„å°cluster
        target_cluster: ç›®æ¨™cluster
        facial_encodings: äººè‡‰ç·¨ç¢¼å­—å…¸
        base_threshold: åŸºç¤é–¾å€¼
        
    Returns:
        bool: Trueè¡¨ç¤ºå®‰å…¨ï¼ŒFalseè¡¨ç¤ºä¸å®‰å…¨
    """
    # å®‰å…¨æª¢æŸ¥1: é¿å…æ¥µç«¯å°ºå¯¸å·®ç•°ï¼ˆé˜²æ­¢noiseè¢«åˆä½µåˆ°ä¸»è¦clusterï¼‰
    size_ratio = len(small_cluster) / len(target_cluster)
    if size_ratio < 0.02 and len(small_cluster) < 5:  # å¤ªå°ä¸”æ¯”ä¾‹æ¥µä½
        print(f"        å®‰å…¨æª¢æŸ¥1å¤±æ•—: clusterå¤ªå°ä¸”æ¯”ä¾‹éä½ ({len(small_cluster)}/{len(target_cluster)})")
        return False
    
    # å®‰å…¨æª¢æŸ¥2: æª¢æŸ¥å°clusterå…§éƒ¨ä¸€è‡´æ€§
    small_internal_similarity = calculate_internal_cluster_consistency(small_cluster, facial_encodings)
    if small_internal_similarity < base_threshold - 0.1:  # å°clusterå…§éƒ¨éƒ½ä¸ä¸€è‡´ï¼Œå¯èƒ½æ˜¯noise
        print(f"        å®‰å…¨æª¢æŸ¥2å¤±æ•—: å°clusterå…§éƒ¨ä¸€è‡´æ€§å¤ªä½ ({small_internal_similarity:.3f})")
        return False
    
    # å®‰å…¨æª¢æŸ¥3: æª¢æŸ¥åˆä½µå¾Œä¸æœƒé¡¯è‘—é™ä½ç›®æ¨™clusterçš„ä¸€è‡´æ€§
    target_internal = calculate_internal_cluster_consistency(target_cluster[:50], facial_encodings)  # æ¡æ¨£50å€‹
    
    # æ¨¡æ“¬åˆä½µå¾Œçš„ä¸€è‡´æ€§
    combined_sample = target_cluster[:25] + small_cluster[:25]  # å„å–25å€‹æ¨£æœ¬
    combined_internal = calculate_internal_cluster_consistency(combined_sample, facial_encodings)
    
    consistency_drop = target_internal - combined_internal
    if consistency_drop > 0.15:  # ä¸€è‡´æ€§ä¸‹é™å¤ªå¤š
        print(f"        å®‰å…¨æª¢æŸ¥3å¤±æ•—: åˆä½µæœƒé¡¯è‘—é™ä½ç›®æ¨™clusterä¸€è‡´æ€§ (ä¸‹é™ {consistency_drop:.3f})")
        return False
    
    print(f"        âœ… é€šéæ‰€æœ‰å®‰å…¨æª¢æŸ¥")
    return True


def calculate_internal_cluster_consistency(cluster, facial_encodings, max_samples=20):
    """
    è¨ˆç®—clusterå…§éƒ¨çš„ä¸€è‡´æ€§ï¼ˆå¹³å‡pairwiseç›¸ä¼¼åº¦ï¼‰
    """
    if len(cluster) < 2:
        return 1.0
    
    # æ¡æ¨£ä»¥é¿å…è¨ˆç®—é‡éå¤§
    sample = cluster[:max_samples] if len(cluster) <= max_samples else \
             np.random.choice(cluster, max_samples, replace=False).tolist()
    
    similarities = []
    for i in range(len(sample)):
        for j in range(i+1, len(sample)):
            sim = np.dot(facial_encodings[sample[i]], facial_encodings[sample[j]])
            similarities.append(sim)
    
    return np.mean(similarities) if similarities else 0.0

def calculate_cluster_center(cluster, facial_encodings):
    """
    Calculate the centroid of a cluster
    
    Args:
        cluster: List of face paths in the cluster
        facial_encodings: Dictionary of facial encodings
        
    Returns:
        Normalized centroid vector
    """
    if not cluster:
        return None
    
    # Get all encodings for the cluster
    cluster_encodings = [facial_encodings[path] for path in cluster]
    
    # Calculate centroid
    centroid = np.mean(cluster_encodings, axis=0)
    
    # Normalize
    centroid_norm = np.linalg.norm(centroid)
    if centroid_norm > 0:
        centroid = centroid / centroid_norm
    
    return centroid

def calculate_inter_cluster_similarity(cluster1, cluster2, facial_encodings):
    """
    Calculate similarity between two clusters using multiple methods
    
    Args:
        cluster1, cluster2: Lists of face paths
        facial_encodings: Dictionary of facial encodings
        
    Returns:
        Dictionary with different similarity measures
    """
    # Method 1: Centroid similarity
    center1 = calculate_cluster_center(cluster1, facial_encodings)
    center2 = calculate_cluster_center(cluster2, facial_encodings)
    
    if center1 is None or center2 is None:
        return {'centroid': 0, 'max_pairwise': 0, 'avg_pairwise': 0}
    
    centroid_sim = np.dot(center1, center2)
    
    # Method 2: Maximum pairwise similarity
    max_sim = 0
    total_sim = 0
    count = 0
    
    sample_size = min(10, len(cluster1), len(cluster2))  # Sample to avoid too much computation
    
    for i, path1 in enumerate(cluster1[:sample_size]):
        for j, path2 in enumerate(cluster2[:sample_size]):
            sim = np.dot(facial_encodings[path1], facial_encodings[path2])
            max_sim = max(max_sim, sim)
            total_sim += sim
            count += 1
    
    avg_sim = total_sim / count if count > 0 else 0
    
    return {
        'centroid': centroid_sim,
        'max_pairwise': max_sim,
        'avg_pairwise': avg_sim
    }

def identify_oversplit_clusters(clusters, facial_encodings, target_large_clusters=None):
    """
    Identify clusters that might be oversplit from large clusters
    
    Args:
        clusters: List of clusters
        facial_encodings: Dictionary of facial encodings
        target_large_clusters: List of cluster indices to focus on (e.g., [2] for cluster 2)
        
    Returns:
        Dictionary of potential merges
    """
    print("ğŸ” Identifying potentially oversplit clusters...")
    
    merge_candidates = {}
    
    # Identify large clusters (>50 faces) as potential "parent" clusters
    large_clusters = []
    for i, cluster in enumerate(clusters):
        if len(cluster) > 50:
            large_clusters.append(i)
            print(f"   Large cluster {i}: {len(cluster)} faces")
    
    # If specific target clusters provided, use those
    if target_large_clusters:
        large_clusters = [i for i in target_large_clusters if i < len(clusters)]
    
    # For each large cluster, find similar small clusters
    for large_idx in large_clusters:
        large_cluster = clusters[large_idx]
        large_center = calculate_cluster_center(large_cluster, facial_encodings)
        
        if large_center is None:
            continue
        
        print(f"\n   Analyzing cluster {large_idx} ({len(large_cluster)} faces)...")
        
        similar_clusters = []
        
        # Check similarity with all other clusters
        for small_idx, small_cluster in enumerate(clusters):
            if small_idx == large_idx:
                continue
            
            # Focus on smaller clusters that might be fragments
            if len(small_cluster) > len(large_cluster) * 0.5:  # Skip clusters that are too large
                continue
            
            small_center = calculate_cluster_center(small_cluster, facial_encodings)
            if small_center is None:
                continue
            
            # Calculate similarity
            centroid_sim = np.dot(large_center, small_center)
            
            # Also calculate cross-cluster similarity
            similarities = calculate_inter_cluster_similarity(large_cluster, small_cluster, facial_encodings)
            
            # Decision criteria for potential merge
            if (centroid_sim > 0.4 or  # Centroid similarity > 0.4
                similarities['max_pairwise'] > 0.6 or  # Some faces very similar
                similarities['avg_pairwise'] > 0.45):  # Average similarity decent
                
                similar_clusters.append({
                    'cluster_idx': small_idx,
                    'size': len(small_cluster),
                    'centroid_sim': centroid_sim,
                    'max_pairwise': similarities['max_pairwise'],
                    'avg_pairwise': similarities['avg_pairwise']
                })
                
                print(f"      Potential merge: Cluster {small_idx} ({len(small_cluster)} faces)")
                print(f"         Centroid sim: {centroid_sim:.3f}")
                print(f"         Max pairwise: {similarities['max_pairwise']:.3f}")
                print(f"         Avg pairwise: {similarities['avg_pairwise']:.3f}")
        
        if similar_clusters:
            # Sort by similarity
            similar_clusters.sort(key=lambda x: x['centroid_sim'], reverse=True)
            merge_candidates[large_idx] = similar_clusters
    
    return merge_candidates

def merge_clusters_intelligently(clusters, facial_encodings, merge_candidates, 
                                merge_threshold=0.45, max_merges_per_cluster=5):
    """
    Intelligently merge clusters based on candidates
    
    Args:
        clusters: Original clusters
        facial_encodings: Dictionary of facial encodings
        merge_candidates: Output from identify_oversplit_clusters
        merge_threshold: Minimum similarity for merging
        max_merges_per_cluster: Maximum number of clusters to merge into one
        
    Returns:
        New merged clusters
    """
    print(f"\nğŸ”§ Performing intelligent cluster merging...")
    
    # Create a copy of clusters
    new_clusters = [cluster.copy() for cluster in clusters]
    merged_indices = set()  # Track which clusters have been merged
    
    merge_actions = []
    
    for parent_idx, candidates in merge_candidates.items():
        if parent_idx in merged_indices:
            continue
        
        print(f"\n   Processing merges for cluster {parent_idx}...")
        
        merges_count = 0
        
        for candidate in candidates:
            if merges_count >= max_merges_per_cluster:
                break
            
            child_idx = candidate['cluster_idx']
            
            # Skip if already merged
            if child_idx in merged_indices:
                continue
            
            # Check if similarity meets threshold
            if (candidate['centroid_sim'] > merge_threshold or
                candidate['max_pairwise'] > merge_threshold + 0.1):
                
                # Merge child cluster into parent
                print(f"      âœ… Merging cluster {child_idx} â†’ {parent_idx}")
                print(f"         Adding {len(new_clusters[child_idx])} faces")
                
                new_clusters[parent_idx].extend(new_clusters[child_idx])
                new_clusters[child_idx] = []  # Empty the merged cluster
                
                merged_indices.add(child_idx)
                merges_count += 1
                
                merge_actions.append({
                    'parent': parent_idx,
                    'child': child_idx,
                    'faces_added': len(clusters[child_idx]),
                    'similarity': candidate['centroid_sim']
                })
    
    # Remove empty clusters
    final_clusters = [cluster for cluster in new_clusters if len(cluster) > 0]
    
    print(f"\nğŸ“Š Merge Summary:")
    print(f"   Original clusters: {len(clusters)}")
    print(f"   Final clusters: {len(final_clusters)}")
    print(f"   Clusters merged: {len(merged_indices)}")
    print(f"   Merge actions performed: {len(merge_actions)}")
    
    for action in merge_actions:
        print(f"   Cluster {action['child']} â†’ {action['parent']}: "
              f"+{action['faces_added']} faces (sim: {action['similarity']:.3f})")
    
    return final_clusters, merge_actions

def post_process_clusters_legacy(clusters, facial_encodings, target_clusters=None, 
                         merge_threshold=0.45, max_merges_per_cluster=5):
    """
    Main post-processing function
    
    Args:
        clusters: Original clusters from clustering algorithm
        facial_encodings: Dictionary of facial encodings
        target_clusters: Specific clusters to focus on (e.g., [2] for cluster 2)
        merge_threshold: Minimum similarity for merging
        max_merges_per_cluster: Maximum merges per parent cluster
        
    Returns:
        Processed clusters and merge report
    """
    print("ğŸš€ Starting cluster post-processing...")
    
    # Step 1: Identify merge candidates
    merge_candidates = identify_oversplit_clusters(
        clusters, facial_encodings, target_clusters
    )
    
    if not merge_candidates:
        print("   No merge candidates found.")
        return clusters, []
    
    # Step 2: Perform merging
    final_clusters, merge_actions = merge_clusters_intelligently(
        clusters, facial_encodings, merge_candidates, 
        merge_threshold, max_merges_per_cluster
    )
    
    return final_clusters, merge_actions

def post_process_clusters(clusters, facial_encodings, strategy='intelligent', **kwargs):
    """
    æ–°çš„çµ±ä¸€post-processingå…¥å£ï¼Œæ”¯æŒå¤šç¨®ç­–ç•¥
    """
    if strategy == 'legacy':
        return post_process_clusters_legacy(clusters, facial_encodings, **kwargs)
    elif strategy == 'intelligent':
        # ä½¿ç”¨æ–°çš„æ™ºèƒ½å…©éšæ®µç­–ç•¥
        total_faces = sum(len(cluster) for cluster in clusters)
        
        # Phase 1
        phase1_clusters, phase1_actions = merge_similar_large_clusters(
            clusters, facial_encodings,
            min_cluster_size=kwargs.get('min_large_cluster_size', max(10, total_faces * 0.02)),
            similarity_threshold=kwargs.get('large_cluster_threshold', 0.55),
            max_cross_sample_check=kwargs.get('max_cross_sample_check', 20)  # æ·»åŠ é€™å€‹åƒæ•¸
        )
        
        # Phase 2
        final_clusters, phase2_actions = merge_small_clusters_intelligently(
            phase1_clusters, facial_encodings,
            small_cluster_threshold=kwargs.get('small_cluster_threshold', total_faces * 0.05),
            merge_threshold=kwargs.get('small_cluster_merge_threshold', 0.45),
            safety_checks=kwargs.get('safety_checks', True)  # æ·»åŠ é€™å€‹åƒæ•¸
        )
        
        return final_clusters, phase1_actions + phase2_actions
    else:
        raise ValueError(f"Unknown strategy: {strategy}")

def save_post_processed_results(clusters, merge_actions, facial_encodings, output_dir):
    """
    Save post-processed results
    """
    print("\nğŸ’¾ Saving post-processed results...")
    
    # Calculate new cluster centers
    from clustering import find_cluster_centers_adjusted
    cluster_centers = find_cluster_centers_adjusted(
        clusters, facial_encodings, method='min_distance'
    )
    
    # Save data
    post_processed_data = {
        'clusters': clusters,
        'facial_encodings': facial_encodings,
        'cluster_centers': cluster_centers,
        'merge_actions': merge_actions,
        'original_cluster_count': len(clusters) + len(merge_actions)
    }
    
    output_path = os.path.join(output_dir, 'post_processed_centers_data.pkl')
    with open(output_path, 'wb') as f:
        pickle.dump(post_processed_data, f)
    
    print(f"âœ… Post-processed data saved to: {output_path}")
    
    return output_path

# Example usage function
def apply_post_processing_to_existing_results(centers_data_path, output_dir):
    """
    Apply post-processing to existing clustering results
    
    Args:
        centers_data_path: Path to existing centers_data.pkl
        output_dir: Directory to save results
    """
    print("ğŸ“‚ Loading existing clustering results...")
    
    # Load existing results
    with open(centers_data_path, 'rb') as f:
        centers_data = pickle.load(f)
    
    clusters = centers_data['clusters']
    facial_encodings = centers_data['facial_encodings']
    
    print(f"   Loaded {len(clusters)} clusters")
    
    # Apply post-processing with focus on cluster 2
    final_clusters, merge_actions = post_process_clusters(
        clusters, facial_encodings, 
        target_clusters=[2],  # Focus on cluster 2
        merge_threshold=0.45,
        max_merges_per_cluster=6  # Allow more merges for cluster 2
    )
    
    # Save results
    output_path = save_post_processed_results(
        final_clusters, merge_actions, facial_encodings, output_dir
    )
    
    return output_path, final_clusters, merge_actions